Andres Loeh, Duncan Coutts

6-7 Feb 2013

## Internals and Performance

### Language Extensions

* Current version: Haskell 2010 (that's what current version of GHC gives out of the box)
* Language extensions: GHC supports over a hundred, there is a consensus in community which ones are standard and stable and which ones experimental
* Recommended way of enabling: `{-# LANGUAGE xxx #-} pragma (also declare in cabal file as "enabled extensions")

### Data Structures

#### Lists

Imperative: destructive update (ephemeral data structures); functinal: creation of a new value (persistent data structures).

`[1,2,3,4]` is syntactic sugar for `1:(2:(3:(4:[])))`

when we do let y =0:x, the x is reused (it's safe to share it, because it's immutable); same when we do `drop 3 y`, it's pointer. However, when we do `take 2`, we have
to copy the cons `(:)` cells (see slides).

#### In-memory representation

Words of memory:

* first identifies the constructor
* other ones contain payload (pointers to constructor args)

~~~
+---+----------+----------+
|(:)|hd ptr -> |tl ptr -> |
+---+----------+----------+
~~~

Thunks are represented similarly, as heap objects:

~~~
+---+----------+----------+
|fn |arg 1 ->  |arg 2 ->  |
+---+----------+----------+
~~~

The first field indicates whether it's a thunk or constructor. If thunk, it can be replaced by evaluated value (one of very few cases where in Haskell something gets changed destructively). The benefit is that if a shared thunk is evaluated, the value will be available to all places pointing to it.

##### Tools

* vacuum -- older, shows evaluated values
* ghc-vis/ghc-heap-view -- lets see the thunks and prod them

useful for visualisation, not so useful for large program debugging

ghc-vis in ghci:

~~~
let x = [1,2,3,4]; w = take 2 x
:vis
:view x
:view w
:switch
-- now we can prod thunks!
:clear
let x = cycle [1,2] -- shows how infinite structures like this are represented
let y = take 5 x -- we need 5 real conses!
let z = drop 3 x
~~~

AP - unevaluated thunk
BCO - bytecode of function
Thunk - another type of thunk (the difference doesn't really matter)

#### Trees

There is a lot of syntactic sugar for lists, but only very few operations on them are efficient. `snoc`, `reverse`, `elem`, `splitAt`, `union` are all linear. They are suitable if all we need are stack operations, if data is small (in the order of hundreds) or if we don't care about performance (data is small). Problematic case: `String` -- it's a list of chars, convenient, but unsuitable for large pieces of text.

Trees are very suitable for persistent, functional setting. Most of persistent data structures are internally represented as trees (cf. binary tries in Scala). Balanced search tree is a sensible representation for data structure that involves lookups.

* `Data.Map`
* `Data.IntMap` (special, efficient case for `Int` keys)
* `Data.Set` (note: values need to be instances of `Ord`)

##### A glimpse at implementation of `Map`

~~~ {.haskell}
data Map k a = Tip
             | Bin {-# UNPACK #-} !Size
                   (Map k a) k a (Map k a)
type Size = Int
~~~

`!` means size will never be a thunk; `UNPACK` changes in-memory representation so that Size is not a pointer, but is stored unboxed.

The library uses *smart constructors* -- wrappers around constructors that maintain invariants (e.g. size)

~~~ {.haskell}
bin :: Map k a -> a -> Map k a -> Map k a
bin l kx x r = Bin (size l + size r + 1) l kx x r
~~~

We have to remember to use smart constructor, not the regular one -- can be ensured by exporting only smart constructors.

~~~ {.haskell}
size Tip              = 0
size (Bin sz _ _ _ _) = sz

insert kx x Tim = singleton kx x
insert kx x (Bin sz l ky y r) =
  case compare kx ky of
    LT -> balance (insert kx x l) ky y
    GT -> balance              l  ky y (insert kx x r)
    EQ > Bin sz l kx x r
~~~

`balance` is another type of smart constructor, even smarter than bin, because it does the balancing. Bonus: rotation (`singleL` and `doubleL`) is easy to write in Haskell!

#### Sequences

* efficient random access
* efficient access to both ends
* efficient concatenation and splitting

e.g. queueus, pattern matching, search and replace etc.

Solution: `Data.Sequence`. Slightly underused, better list. List still has its uses due to syntax, conceptual simplicity and adequacy for small data sets and possibilit to be infinite.

How is it implemented? Take a tree with sequence elements in leaves; then invert it so that start and end are treated as "roots" of the tree, and the pointers are reversed. In the end we get finger tree, with strong trunk a lot of branches (start and end are on the first level).

Check ghc-viz :view `Data.Sequence.fromList([1..30])` -- note that we can see unpacked values in constructor nodes, not as pointers

~~~ {.haskell}
newtype Seq a = Seq (FingerTree a)
data FingerTree a = Empty
                  | Single a
                  | Deep {-# UNPACK #-} !Int
                         !(Digit a) (FingerTree (Node a)) !(Digit a)

data Node a = Node2 {-# UNPACK #-} !Int a a
            | Node3 {-# UNPACK #-} !Int a a a

data Digit a = One a | Two a a | Three a a a | Four a a a a
~~~

#### Arrays

Constant time access to data and compact storage. `array` package is the old solution, `vector` is new favourite.

`let x = fromList [1,2,3,4,5] in x // [(2,13)]`

The price to pay is that we need to copy the entire array on update -- that's why we can do bulk updates, `(//)` where updated values are specified as list. Because of this arrays are used much less in functional languages than in imperative languages. Ideal use cases: 

1. construct once, use many times
2. update all elements at once

`Data.Vector` operations: `slice` is constant time, but `(++)` is linear. Vectors can be constructed `fromList` and using generating function (`generate`).

Arrays are special in that we can't define them using `data`, they have to be built into the compiler. In GHC it's a heap object consisting of descriptor, length and a number of words containing data (see in ghc-vis, we'll see that actual sizes are powers of 2).

#### Unboxed Types

ghci:

~~~
> :i Int
data Int = GHC.Types.I# GHC.Prim.Int#
~~~

what is `Int#`?

~~~
> :i GHC.Prim.Int#
data GHC.Prim.Int# -- Defined in `GHC.Prim'
~~~

`#` is to scare people. To get names like these to parse we need `MagicHash` extension. `Int#` is real machine integer. So Int has a tag I# and unpacked machine integer. Heap representation:

~~~
           +----+----+
x: Int ->  | I# | 3# |
           +----+----+
~~~

x is a boxed integer; 3# is unboxed integer. By default everything is boxed, compiler will unbox if possible and appropriate. We can force unpacked values to be stored in our data structure using the pragma `{-# UNPACK #-}` and strictness annotation -- see earlier examples. Pragma and annotations are separate because pragma should not introduce sematic changes and strictness is a semantic change. There's a flag `-funbox-strict-fields` that will attempt to unbox all strict fields.

Why do we want to box? It admits laziness and polymorphism. Unboxed ints are good when we just use them in comparison, but stop to be an optimisation when we later use them in operation that would need to re-box them.

Aside: specialisation is normally done explicitly (`Map` and `IntMap`), but there are advanced techniques for implicit specialisation (used e.g. in `Vectors`).

~~~ {.haskell}
3#    :: Int#
3##   :: Word#
3.0#  :: Float# 
3.0## :: Double#
'c'#  :: Char#
~~~

also functions, e.g. `(+##)` for doubles.

Kinds are also incompatible:

~~~ {.haskell}
Int :: *
[] :: * -> *
Int# :: #
~~~

so we can't have a list of `Int#`s. Fortunately, we very rarely use unboxed types directly, GHC deals with it for us. We can tell what unpacking was applied by looking at core. Any type that has a *single* data constructor can be unpacked; this can affect how we choose to represent data, if we care about low-level efficiency.

`Data.Vector.Unboxed` provides more compact and more local storage, but restricts the types of elements we can store. There is a type class `Unbox` that describe things that can be stored in it. Interesting case:

~~~ {.haskell}
instance (Unbox a, Unbox b) => Unbox (a, b)
~~~

The vector representation in this case is actually a pair of vectors instead of vector of pairs!

Interesting trick with normal arrays: values can depend on each other, like in a spreadsheet -- a way to do dynamic programming. Not feasible in case of unboxed vectors. That's why compiler will not change the type of compiler for us -- it changes the semantics.

#### Mutable Vectors

There are (few) algorithms that can be implemented more efficiently with destructive updates (e.g. DFS). Haskell offers mutable arrays and most oprations on them are in `IO`. Packages: `Data.Vector.Mutable` and `Data.Vector.Unboxed.Mutable`.

~~~ {.haskell}
data IOVector -- abstract

new :: Int -> IO (IOVector a)
read :: IOVector a -> Int -> IO a
write :: IOVector a -> Int -> a -> IO ()
~~~

#### Strings

~~~ {.haskell}
type String = [Char]
~~~

"foo" takes 15 words to represent in memory (see list representation).

`ByteString` and `Text` (newer) offer compact representations, like unboxed vectors. There are also lazy versions of these, suitable for huge strings that should not be held in memory completely. `Text` is UTF16, `ByteString` is a byte array, without encoding implied. `ByteString ` is appropraite for ASCII, `Text` for Unicode.

Use case: reading infinite stream from socket: `Data.ByteString.Lazy` and then use `hGetContents`. In network protocols it's better to be more explicit, i.e. read line by line and have a chance to respond.

#### Summary

* sets, maps and sequences are good general-purpose structures
* lists are good for small data and incremental streaming
* vectors and arrays are useful in some cases, but need to be careful (copying) 


### Lazy Evaluation and Profiling

We don't need to understand evaluation to write semantically correct programs, but it's crucial for performance.

#### Reduction

_Redex_ is an expression that can be reduced. Typically replacing lhs of fun definition by right hand side. How does Haskell choose which redex to reduce?

~~~
id (id (\z -> id z))
~~~

there are three possible reductions.

~~~
head (repeat 1)
~~~

Can't reduce head because it needs to know if thing is non-empty. We can reduce `(repeat 1)` and then we can reduce `head`. Here the choice of reduction makes a difference between termination and non-termination.

~~~
let minimum xs = head (sort xs) in minimum [4,1,3]
~~~

Here the redex choice makes a difference in performance.

Rules in haskell:

* leftmost outermost redex is chosen first
* sharing is introduced: whenever a name is bound to an expression

When no redexes are left, expression is in *normal form*. If top-level of an expression is a constructor or lambda, the expression is in *(weak) head normal form* (even if inside there are still redexes). Weak head normal form is important because it signifies progress in our program.

Evaluation strategies:

* *call by value*/eager (strict) evaluation -- most common. Args are reduced as far as possible before function application
* *call by name* -- not very often, occurs in some macro languages e.g. (TeX). Function is reduced before their arguments, so evalution of args can be run multiple times
* *call by need*/lazy evaluation -- optimised "call by name". Args are only reduced when needed, but shared if used multiple times.

Example of sharing: `\f g x -> combine (f x) (g x)`

From Church-Rosser theorem, each term has at most one normal form -- the order in which redexes are chosen does not affect semantics, only termination/non-termination and performance.

Also: if a term has a normal form, lazy evaluation will always find it, while strict might loop, e.g. `head (repeat 1)`.

Example: the first 100 odd square numbers

~~~ {.haskell}
example :: [Int]
example = (take 100 . filter odd . map (\x -> x * x)) [1..]
~~~

Good thing: lets us separate generation from consumption; in this case we don't know in advance how many numbers we have to generate.

What drives evaluation?

* On top level, in ghci, printing to string.
* In function, pattern matching; we have to evaluate enough of arguments to know which pattern to choose. That's where WHNF comes into play.

##### Tracking demand

`const x y = x` -- demanding the result demands first arg, but not second

~~~ {.haskell}
True  || _ = True
False || y = y
~~~

will demand first arg, and if it's false, will then demand the second.

We read it backwards: assume the result of the function is depanded and then see what args are required for that. Note that form of function definition has implications for runtime:

~~~ {.haskell}
True  || True = True
True  || False = True
False || True = True
False || False = False
~~~

that would have different properties -- would demand evaluation of both args (no short ciruit!).

~~~ {.haskell}
map f [] = []
map f (x:xs) = f x : map f xs
~~~

This will not demand `f`.

What are other ways for tracking demand? 

###### Bottoms

Feed non-terminating (or undefined) computation into a function and demand its result. Note: if it loops we can't be sure if it was our bomb, or some other problem in the function

Strictness: a function f is called strict iff `f _|_ = _|_`. What follows is a useful optimisation technique: when calling strict functions, it's safe to evaluate the argument before applying the function to it. Compiler does a lot of it; it's allowed because 1) Haskell standard only mandates non-strict semantics, not lazy evaluation and 2) we consider all bottoms equal. Aside, regarding the last point: use `error` for programming errors; for validation use exceptions -- here we care about what error it was.

In a strict language, all functions are strict. In Haskell we have both strict and non-strict functions.

###### Trace

`Debug.Trace`

~~~ {.haskell}
trace :: String -> a -> a
traceShow :: Show a => a -> b -> b
~~~

Provided first arg will be printed when the second arg is demanded (evaluated).

Try the cons/nil trace exercise.

#### Data in Memory

Suspended computations (thunks) are created on the heap. These can be shared just as other subterms. There are subtleties with respect to parallel evaluation, but other than that it's a straightforward pointer replacement when thunk is evaluated, and all users can see the calculated value.

GHC uses generational GC, optimised for lots of short-lived data. Young generation is small (half of L2 cache by default) and collected often. The size of the heap grows and shrinks as required. With lazy evaluation it's hard to predict how long things are held on to -- cause of space leaks.

~~~ {.haskell}
sum1 [] = 0
sum1 (x:xs) = x + sum1 xs
~~~

This will put all of these numbers on stack before they can be summed up. Not heap, because function args are put on stack, not heap.

There are useful RTS options for tracking what's happening. In particular: `-t`, `-s` and `-S`. With `-s` we can see the proportion of time spent in GC (MUT is our program!). Other interesting metrics: maximum residency -- how much memory was needed at once. Gen 0 is young, Gen 1 is old.

Useful tool: ekg gives these stats live!

#### Heap Profiling

need to compile all libraries with profiling, best add `library-profiling: True` to cabal-install config.

Compile: `ghc --make -prof -auto-all -rtsopts Prog`
Run: `Prog +RTS -hc`

`-hc` is for const-centre heap profiling. It then produces .prof and .hp files, the latter can be rendered using hp2svg or hp2ps. Note: amount of live data is shown in the graph, this will be much smaller than total heap size.

----

Improvement of our sum (via tail recursion):

~~~ {.haskell}
sum2 xs = go 0 xs
  where go n [] = n
        go n (x:xs) = go (n + x) xs
~~~

But heap profile looks even worse! That's because all additions are allocated as thunks. So we first build huge list of thunks, and then reducing this chain wil take lots of stack space.

What we really want to do is evaluate `(n + x)` immediately! In fact, if we enable `-O` compiler will do strictness analysis and will evaluate it. However, in more complex scenarios that might not be the case. We need more control over evaluation.

#### Forcing evaluation

~~~ {.haskell}
seq :: a -> b -> b -- primitive
~~~

When second arg is demanded, the first one will be demanded as well. Sane uses of `seq`: first arg is a variable. For it to make sense there needs to be some sharing between firsst and second arg of `seq`.

why `seq :: a -> b -> b` and not `seq :: a -> a`? Because the latter would have been a noop

With this we can define "strict" application:

~~~ {.haskell}
($!) :: (a -> b) -> a -> b
f $! x = x `seq` f x
~~~

Note it will only evaluate x to WHNF, not fully. See interesting quiz in the slides: it illustrates that there is a lot of subtlety in where the bottom is in the structure -- that's a complexity of lazy languages that strict don't have.

So, how do we fix our sum?

~~~ {.haskell}
sum3 xs = go 0 xs
  where go n [] = n
        go n (x:xs) = (go $! n + x) xs
~~~

Note: `n + x 'seq' go (n + x) xs` won't work becuase there's not sharing between the two additions!

Now the heap profile looks reasonable.

Note: 

~~~ {.haskell}
sum1 = foldr (+) 0
sum2 = foldl (+) 0 -- never us it! foldl' is (almost) never worse, often much better
sum3 = foldl' (+) 0
~~~

`foldr` can be e.g. used to implement `map` lazily. In general: if our function is lazy in recursive call, use `foldr`, if it's strict, use `foldl'`.

#### Bang patterns

~~~ {.haskell}
{-# LANGUAGE BangPatterns #-}

sum4 xs = go 0 xs
  where go !n [] = n
        go !n (x:xs) = go (n + x) xs
~~~

The `!` tells the runtime that go is strict in first argument. Strictly speaking, we only need it in the second case, but convention is to put it everywhere. It's easier to understand than `sum3`. `!` only makes sense in front of a variable, not type constructor (because that would cause strictness anyway).

Why we want bang patterns and not rely on strictness analysis?

~~~ {.haskell}
sum5 xs = go 0 xs
  where go n [] = 0
  		go n [x] = x + n
        go n (x:xs) = go (n + x) xs
~~~

Here compiler strictness analysis doesnt's work, because the first case does not use n!

#### Strictness analysis

~~~ {.haskell}
sum2 xs = go 0 xs
  where go n [] = n
        go n (x:xs) = go (n + x) xs
~~~

compile with `-ddump-simpl` and behold the core language output (beware of dictionaries for type classes such as `Num`!). If we now run the optimised one, we'll see that function arg is unboxed type, which is by necessity strict. In core, the type of things tells us the calling convention. If we wanted to check strictness for non-primitive type, there is `[Str=DmdType LS]` annotation which tells us demand type for args (L for lazy, S for strict). Also if there is a case in core, the arg affected will always be strictly evaluated.

#### Irrefutable (lazy) patterns

~~~ {.haskell}
unzip :: [(a, b)] -> ([a], [b])
unzip = foldr (\(x, y) (xs, ys) -> (x:xs, y:ys)) ([], [])
~~~

This results in stack overflow for large lists. Remember what we said about `foldr`? It should be used with functions lazy in the second arg, but here we have a pair, i.e. pattern match, i.e. strict. Here we know that the arg will always be a pair, so we could rewrite our function as:

~~~ {.haskell}
unzip = foldr (\(x, y) xys -> (x:(fst xys), y:(snd xys))) ([], [])
~~~

and that would fix it, but it complicates the definition somewhat. We can do instead:

~~~ {.haskell}
unzip = foldr (\(x, y) ~(xs, ys) -> (x:xs, y:ys)) ([], [])
~~~

`~` delays evaluation, even though there is a pattern match. Can only be used in front of a constructor where there is only one case in the pattern match. In principle, should only be used for single constructor data types. Comes up infrequently, bang patterns are much more used.

If a pattern is in let/where, it's implicitly irrefutable: `let (xs, ys) = unzip xys in xys`. This can be undone with `!`. Case, on the other hand, is strict by default, so here we can use `~`.

#### Control.DeepSeq

`seq` evaluates to WHNF. When it's not sufficient, we can use

~~~ {.haskell}
deepseq :: NFData a => a -> b -> b
($!!) :: NFData a => (a -> b) -> a -> b
force :: NFData a => a -> a
~~~

Note: here `force` makes sence to. Unfortunately `NFData` is not derivable, but definition of instances is mechanical. 

~~~ {.haskell}
instance NFData a => NFData [a] where
  rnf []     = ()
  rnf (x:xs) = rnf x `seq` rnf xs
~~~

Be careful with `deepseq` -- unlike `seq`, it can be expensive to run when unnecessary.

Advice: make accummulating args strict, otherwise don't worry about strictness of arguments. When necessary, use strict data structures instead. Certainly don't sprinkle bangs everywhere, it can make matters worse!

