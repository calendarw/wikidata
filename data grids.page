## Scaling the system

### Stateless middle tier

Adding more CPUs to a blade does not always help: when blades upgraded from 8 to 24 CPUs many users didn't see improvement due to resource contention and non thread-safe code that earlier worked fine. In many cases poopular solution is horizontal scaling. Usual solution to horizontal scaling:

~~~ {.dot}
digraph g {
"load balancer" -> node1;
"load balancer" -> node2;
"load balancer" -> node3;
node1 -> database;
node2 -> database;
node3 -> database;
}
~~~

nodes are stateles, database is stateful. Scaling the middle tier is easy, database is hard (e.g. RAC).

Database scaling patterns:

* master/slave -- all writes to master, replication from master to slaves, slave replicas only for reading
* clustering -- multi-way replication
* sharding (partitioning) -- based on data keys, e.g. by region/customer etc. does not provide HA on its own, expensive to maintain.

Other option: no ACID, e.g. eventual consistency.

### Stateful middle tier

What if we want *stateful* middle tier? This is where data grids come into play.

Examples of common caches:

* Hibernate session cache and L2 cache (can span multiple sessions)
* Entity Bean cache (EJB 2.1)
* JPA cache
* various custom and open source caches

Typically they are used to cache database data. Data grids are cache on steroids.

Custom local caching:

* JCache (JSR 107) standardises the API
* easy to implement using `ConcurrentHashMap`

Challenges:

* eviction
* loading
* prefetching
* write-behind processing
* clustering (this one is hard!)

Basic API (JCache):

* `put(K, V)`
* `get(K)`

Common cache interaction patterns:

* *cache-aside*. When reading from a data store we store retrieved data in cache and look there first.
* *read-through* -- it's the responsibility of cache to read from data store if it's not in cache
* *write-through* -- same as read, just in opposite direction
* *write-behind* -- like *write-through*, but the write is asynchronous

Clustered cache brings in additional challenges:

* what to do on an update? Common answers are update (send entry to replica) or invalidate (cheaper)

Replication (in [Coherence]()):

* with replication, a reasonable limit is 4 cache nodes
* ideally all replication traffic should be on its own network
* instead of storing complete copy in every node a better strategy is partitioning: even if we ask a node that doesn't have the data, it will know which node to obtain it from, it's transparent to the user code. This is done by hashing a key to a bucket, buckets in turn are assigned to nodes. These buckets can be reallocated when the cluster changes.
* with one copy of the value it's not HA. We can configure the cache to have multiple copies of the data: a primary owner and a backup. If one of these goes offline Coherence will create a new copy to maintain redundancy.

Q: Coherence deployment topologies:

* embedded (in same JVM as the app) -- straightforward, cheap, not elegant, only suitable for local development
* storage-disabled (same node but separate JVM) -- safer, but still susceptible to upsetting of cluster e.g. due to app upgrade
* client-server -- app just has client jars, cache tier is completely separate, can also use Coherence proxy nodes in front of storage nodes.

## Coherence at scale

Coherence is expensive ("the most expensive 18 MB"), we want to run many more than four nodes. Client-server deployment makes a lot of sense, it allows multiple applications to use the same cache. 

* linear scalability: 2 hops for read and write (app -> proxy -> node).
* HA: configurable number of duplicates

Example cost (21 Amazon xLargeMemory instances): $9000/month/125GB

What when data doesn't fit? Data can overflow to:

* memory-mapped file
* off-heap byte array
* Berkeley DB

Ensuring HA: push replication between data centres

Can we ditch the database? Pretty much, we only need to store mandatory audit info (that allows replay), the rest can live in the grid.

### Processing data

Challenge: pulling the data back to application can saturate the network. Solutions: 

* computation on the grid
* near cache, that contains the "working set"
* in-place processing: `EntryProcessor`, `Aggregator` (i.e. map-reduce) -- handles failover transparently
* running arbitrary `Runnable` on the grid -- does not handle failover, not very popular

### Querying

Queries are executed in parallel on the nodes. Since coherence 3.6: JPQL-like query language.

### Events

We can register listeners for events and perform any actions we want in response.

Continuous query cache: like near cache, but its contents are based on a query and continuously updated. 

### Coherence*Extend

i.e. client-server mode: extend client connects to remote cache.

Q: stability of incubator projects?

A: push replication very stable, is going to be incorporated into the main product

Q: handling conflicts in push replication?

A: Currently we need to provide a resolved that will resolve conflicting updates. Users have been dealing with this by using different keys in different areas and then resolving in application logic.

## Lab

### Basic cluster startup

* logged in to EC2 instance
* updated "well-known-addresses" in `tangosol-coherence-override.xml` to point to a predefined host for the cluster -- necessary if env does not support UDP multicast, as on AWS EC2.
* problems with cluster nodes talking to each other -- down to EC2 security group
* coherence client

~~~
cache ec2-cache
put "mm" "sample value"
size
list
~~~

* shut down the node

### Querying

~~~
ensure cache 'ec2-cache';
select * from 'ec2-cache';
insert into 'ec2-caache' key 'mm2' value 'another value';
~~~

### Updates

* stock ticker (with polling) and price updater sample apps

### Events

* chat sample app

When attaching new nodes to existing cluster we've been asked to do it one at a time, not all at once, "just in case".

Demo of rebucketing: when new nodes join they data is redistributed evenly.

## Q&A

Q: invalidation in the presence of independent writes to database?

A: have to be managed on your own, or use Golden Gate
