---
categories: statistics
...

ANOVA = analysis of [variance](). Most appropriate when all independent variables are categorical and the dependent variable is continuous. Common application is analysis of data from randomised controlled experiments. When there are just two independent variables a [t-test]() is simpler.

It typically involves [NHST](null hypothesis significance testing):

$F=\frac{MS_{between}}{MS_{within}}$

Where $MS_{between}$ is variance (mean squres) between groups (the "good variance", systematic) and $MS_{within}$ is variance within groups (the "bad variance", non-systematic). Null hypothesis is that $F$ is 0.

## Effect size

$\eta^2=\frac{SS_A}{SS_{total}}$ -- the percentage of variance in dependent variable explained by the independent variables. ($SS$ -- sum of squares, TODO)

## Assumptions

As for independent [t-test](): 

* dependent variable is continuous
* dependent variable is normally distributed
* *within-group* variance is homogenous between groups (can be tested with Lavine's test)

## *Post-hoc* tests

If ANOVA returns a significant result that only tells us that some of the groups compared were different, but doesn't say how many or which ones. Post-hoc tests are used to make pairwise comparisons. Special procedures are required for such comparisons, since a doing it naively (i.e. with [t-tests](t-test) would inflating the probabilitites and lead to [type I error](null hypothesis significance testing)). There is a number of procedures, a commonly used one is Tukey's. Bonferroni's is the most conservative.

## Factorial ANOVA

When there is more than one independent variable. Example: 

independents: 

* driving difficulty
* conversation demand

dependent:

* driving errors

There will be three $F$ ratios: $F_{dd}$, $F_{cd}$, $F_{dd\times cd}$.

*Simple effect*: the effect of one IV at a particular level of other IV. If it changes as a result of changes in the other IV then there is interaction.

*Main effect*: the effect of one IV averaged across the levels of the other IV.

*Interaction effect*: when effect of one IV depends on the other. Like [moderation in multiple regression](linear regression).

When $\eta^2$ is calculated it can be calculated as partial or complete. Partial removes effects of individual variables and only leaves the interaction.

If there is significant interaction we should do simple effect analysis, e.g. simple effect of driving difficulty at each level of conversation demand. This can be done as a series of [t-tests](t-test).

## Repeated Measures

When the same subject is measured multiple times, as in [dependent t-test](t-test). Customarily line plots are used instead of bar plots, to signify that it's the same subject measured multiple times.

## In R

~~~ {.r}
a <- aov(dependentVar ~ independentVar)
summary(a)

# repeated measures:
aov(dep ~ indep + Error(factor(subject)/indep)))
~~~

## References

* Statistics 1 Coursera course [lecture](https://class.coursera.org/stats1-002/lecture/83)