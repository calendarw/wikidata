# Making Big Data Small: Streaming Algorithms in Scala

2012-09-12, Noel Welsh

**B**ig **D**ata? That is Hadoop and map/reduce. We're talking here about something different: data sets bigger than our resources can handle in one go.

### Demands

* scalable
* reak time (not batch, like Hadoop)
* simple to implement

### Streaming Algorithms

* process data in one pass
* have small space requirements (e.g. O(log(log n)))
* limited computation requirements per item (typically O(1))

But:

* give approximate answers

### Hash Functions

* deterministic
* uniform distribution
* bit values are independent (any subset of bits is equally good approximation)

In practice, use Murmur Hash 3. Scala 2.10 provides `scala.util.hashing`, look into Guava if we more algorithms are needed.

### [Bloom Filters](Bloom filter)

* a set (i.e. have I seen this user before?)
* ~5 times smaller than corresponding hash set
* there is a chance of false positives ("in set" answer even though item is not in set), but guarantee of no false negatives

Idea: bit set. _index = hash(data) mod m_; bit at index represents the item. But: there will be many ccllisions. How do we improve the confidence? Use multiple has functions:

_i<sub>1</sub> = h<sub>1</sub>(data) mod m_

_i<sub>2</sub> = h<sub>2</sub>(data) mod m_

More hashing functions -> better chance of no false positives. We can adjust the probability by setting the size of bit array, i.e. bits per element. E.g. to get 0.05 probability of false positive we need 6.2 bits/element.
