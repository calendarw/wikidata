## Supervised

Supervised machine learning involves first training the model on a training set of data, that is already labelled with expected results (e.g. classification). Some concrete techniques follow, illustrated by a tutorial example from [Kaggle](http://kaggle.com) -- Titanic survivors.

### Rules

E.g.: if woman then survived (TODO: more details).

### Decision Trees

Making a decision based on values of various variables. It makes sense to put the variable that is the best classifier (i.e. has the lowest [entropy]()) at the root, and follow down by choosing the most discriminating of remaining variables.

Example:

~~~ {.dot}
digraph titanic {
  node [shape = plaintext];
  gender -> age [label=male];
  gender -> class [label=female];
  age -> survived [label="< 15"];
  age -> drowned [label=">= 15"];
  class -> survived [label=first];
  class -> survived [label=second];
  class -> drowned [label=third];
}
~~~

### Bagging

TODO

### Boosting

Iterative re-training where the points misclassified during verification are added to the training set with large weights.

### Bootstrapping

A technique for generating multiple training sets from a single one by randomly drawing (with repetition) data points. Allows to train multiple classifiers in parallel; these can then be used toegether (with majority voting or averaging) to obtain better results.

## Unsupervised

There is no feedback.

### Clustering

Grouping data points into sets. Popular technique: [k-means](). Also recommended is [DBSCAN]() which in many situations gives better results.

## References

* an article in CACM 10/2012.
* a set of lectures in Coursera's [Introduction to Data Science](https://class.coursera.org/datasci-001/lecture/index)