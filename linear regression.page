---
categories: statistics
...

A statistical technique for predicting values of random variable assuming its relationship with underlying variable is linear. _Simple regression_ refers to a situation where there is just one underlying variable; if there are multiple we are talking about _multiple regression_.

## Formula

### Simple Regression

If we are predicting variable $Y$ from $X$:

$Y=B_0+B_1X_1+e$

When $Y$ and $X$ is given we can solve for $B_0$ and $B_1$. $B_0$ -- regression constant, $B_1$ -- regression coefficient.

$B_1=r\times \frac{\sigma_Y}{\sigma_X}$

Where $r$ is the [Pearson product-moment correlation coefficient]().

TODO: how is $B_0$ calculated? Using [ordinary least squares estimation]()?

### Multiple Regression

In simple regression we had one predictor ($X$), in multiple we can have many:

$Y=B_0+\sum\limits_{i=1}^k B_iX_i+e$

How are Bs calculated? Assuming standardised values $B_0$ is 0, $B$ is a vector of $k$ coefficients (i.e. $k\times 1$ matrix), $\hat{Y}$ is a vector of $n$ predicted values and $X$ is a matrix $n\times k$ of values of the predictor variables, then:

$\hat{Y}=BX$

To solve for B, take $Y$ instead of $\hat{Y}$ (which is unknown) and pre-multiply both sides by $(X^TX)^{-1}X^T$:

$B=(X^TX)^{-1}X^TY$


## Interpretation

The regression constant is the predicted value when all Xs are 0. the regression coefficient $B_i$ is the increase of $Y$ per unit increase in $X_i$ when all other Xs are at their average values. Therefore we need to be careful not to make too far reaching inference from these coefficients. E.g. if the sample is skewed in distribution of some of the variables we can't really say much about the correlation between $Y$ and $X_i$ -- as in the lecture, where the question whether male or female professors have higher salary could not be anwered based on the model because it could be that most of the women had shorter tenure than men.

## Correlation

The correlation between values predicted by regression model and actual values is denoted by $R$. Then $R^2$ is the percentage of variance in $Y$ explained by the regression model.

## Centering Predictors

Regression constant $B_0$ is the predicted value when all predictor variables are 0. This is usually not meaningful because of the scale of the variables (e.g. age). We can center the predictors by taking $X' = X - M_X$ and then the constant will tell us predicted score at average values of all predictors.

Centering is also necessary if we want to preserve the interpretation of coefficients as change in predicted variable per unit of predictor at mean levels of other predictors in the presence of [moderation](moderator variable).

Another reason for centering: helps avoid multicolinearity, i.e. two predictors being very highly correlated, which causes numerical issues in matrix computations when solving regression equation.

## Mediation

Sometimes one of the variables helps explain the outcome; e.g. when we are measuring the effect of a threat on IQ, we can also measure the effect on working memory, and thus explain the IQ results:

1. `lm(IQ ~ threat)`
2. `lm(working_memory ~ threat)`
3. `lm(IQ ~ threat + working_memory)`

If the results of models 1. and 2. are significant and in model 3. `threat` is not significant then `working_memory` is the mediator.

In a graphical model this can be shown as:

~~~ {.dot}
digraph G {
  X [shape=rect];
  Y [shape=rect];
  M [shape=rect];
  1 [shape=triangle];
  X -> Y [label="B_1"];
  1 -> Y [label="B_0"];
  e_1 -> Y [label="1"];
  e_2 -> M [label="1"];
  X -> M;
  M -> Y;
}
~~~

## in R

A simple example, after [http://msenux.redwoods.edu/math/R/regression.php]():

~~~ { .r }
age <- 18:29
height <- c(76.1, 77, 78.1, 78.2, 78.8, 79.7, 79.9, 81.1, 81.2, 81.8, 82.8, 83.5)
# find coefficients:
model = lm(height ~ age)
# show the summary
summary(model)
# plot the points:
plot(age, height)
# plot the line:
abline(model)
~~~

A plot with confidence intervals:

~~~ {.r }
# if required: install.packages("ggplot2")
library("ggplot2")
# TODO: construct data from age and height
ggplot(data, aes(x = age, y = height)) + geom_smooth(method = "lm") + geom_point() 
~~~

## References

* Statistics 1 Courser course [lectures](https://class.coursera.org/stats1-002/lecture/index) 7, 11 and 13